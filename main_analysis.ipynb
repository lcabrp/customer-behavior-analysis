{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1137176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_analysis.py\n",
    "\n",
    "# Part 1: Import Statements\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "#from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "#plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca8cd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'petroff10', 'seaborn-v0_8', 'seaborn-v0_8-bright', 'seaborn-v0_8-colorblind', 'seaborn-v0_8-dark', 'seaborn-v0_8-dark-palette', 'seaborn-v0_8-darkgrid', 'seaborn-v0_8-deep', 'seaborn-v0_8-muted', 'seaborn-v0_8-notebook', 'seaborn-v0_8-paper', 'seaborn-v0_8-pastel', 'seaborn-v0_8-poster', 'seaborn-v0_8-talk', 'seaborn-v0_8-ticks', 'seaborn-v0_8-white', 'seaborn-v0_8-whitegrid', 'tableau-colorblind10']\n"
     ]
    }
   ],
   "source": [
    "# Seaborn styles\n",
    "print(plt.style.available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc7dff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:44:09,078 - src.data_acquisition - INFO - Processing raw data...\n",
      "2025-06-03 21:44:09,079 - src.data_acquisition - ERROR - Error downloading Kaggle dataset: No module named 'kaggle'\n",
      "2025-06-03 21:44:09,341 - src.data_acquisition - INFO - Generated synthetic data: 50000 transactions, 1000 customers\n",
      "2025-06-03 21:44:09,541 - src.data_acquisition - INFO - Data preprocessing complete!\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "Error binding parameter 6: type 'Period' is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mProgrammingError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 3. Data acquisition and processing\u001b[39;00m\n\u001b[32m     16\u001b[39m transactions_df, customers_df = da.load_and_preprocess_data()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_sqlite_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransactions_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustomers_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 4. Data processing and feature engineering\u001b[39;00m\n\u001b[32m     20\u001b[39m enhanced_df = processor.engineer_features(transactions_df, customers_df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\lcabr\\GitHub\\customer-behavior-analysis\\src\\data_acquisition.py:189\u001b[39m, in \u001b[36mDataAcquisition.create_sqlite_database\u001b[39m\u001b[34m(self, transactions_df, customers_df)\u001b[39m\n\u001b[32m    186\u001b[39m conn = sqlite3.connect(db_path)\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# Save DataFrames to SQLite\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[43mtransactions_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtransactions\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreplace\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m customers_df.to_sql(\u001b[33m'\u001b[39m\u001b[33mcustomers\u001b[39m\u001b[33m'\u001b[39m, conn, if_exists=\u001b[33m'\u001b[39m\u001b[33mreplace\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# Create indices for better performance\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\lcabr\\GitHub\\customer-behavior-analysis\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\lcabr\\GitHub\\customer-behavior-analysis\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:3087\u001b[39m, in \u001b[36mNDFrame.to_sql\u001b[39m\u001b[34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[39m\n\u001b[32m   2889\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2890\u001b[39m \u001b[33;03mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[32m   2891\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3083\u001b[39m \u001b[33;03m[(1,), (None,), (2,)]\u001b[39;00m\n\u001b[32m   3084\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m   3085\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sql\n\u001b[32m-> \u001b[39m\u001b[32m3087\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3088\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3090\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3091\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3092\u001b[39m \u001b[43m    \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[43m=\u001b[49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3093\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3094\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3095\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3098\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\lcabr\\GitHub\\customer-behavior-analysis\\.venv\\Lib\\site-packages\\pandas\\io\\sql.py:842\u001b[39m, in \u001b[36mto_sql\u001b[39m\u001b[34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[39m\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    838\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mframe\u001b[39m\u001b[33m'\u001b[39m\u001b[33m argument should be either a Series or a DataFrame\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    839\u001b[39m     )\n\u001b[32m    841\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con, schema=schema, need_transaction=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[43m=\u001b[49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\lcabr\\GitHub\\customer-behavior-analysis\\.venv\\Lib\\site-packages\\pandas\\io\\sql.py:2851\u001b[39m, in \u001b[36mSQLiteDatabase.to_sql\u001b[39m\u001b[34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, engine, **engine_kwargs)\u001b[39m\n\u001b[32m   2841\u001b[39m table = SQLiteTable(\n\u001b[32m   2842\u001b[39m     name,\n\u001b[32m   2843\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2848\u001b[39m     dtype=dtype,\n\u001b[32m   2849\u001b[39m )\n\u001b[32m   2850\u001b[39m table.create()\n\u001b[32m-> \u001b[39m\u001b[32m2851\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\lcabr\\GitHub\\customer-behavior-analysis\\.venv\\Lib\\site-packages\\pandas\\io\\sql.py:1119\u001b[39m, in \u001b[36mSQLTable.insert\u001b[39m\u001b[34m(self, chunksize, method)\u001b[39m\n\u001b[32m   1116\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1118\u001b[39m chunk_iter = \u001b[38;5;28mzip\u001b[39m(*(arr[start_i:end_i] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m data_list))\n\u001b[32m-> \u001b[39m\u001b[32m1119\u001b[39m num_inserted = \u001b[43mexec_insert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[38;5;66;03m# GH 46891\u001b[39;00m\n\u001b[32m   1121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_inserted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\lcabr\\GitHub\\customer-behavior-analysis\\.venv\\Lib\\site-packages\\pandas\\io\\sql.py:2547\u001b[39m, in \u001b[36mSQLiteTable._execute_insert\u001b[39m\u001b[34m(self, conn, keys, data_iter)\u001b[39m\n\u001b[32m   2545\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_execute_insert\u001b[39m(\u001b[38;5;28mself\u001b[39m, conn, keys, data_iter) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m   2546\u001b[39m     data_list = \u001b[38;5;28mlist\u001b[39m(data_iter)\n\u001b[32m-> \u001b[39m\u001b[32m2547\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecutemany\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minsert_statement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m conn.rowcount\n",
      "\u001b[31mProgrammingError\u001b[39m: Error binding parameter 6: type 'Period' is not supported"
     ]
    }
   ],
   "source": [
    "# main_analysis.ipynb\n",
    "\n",
    "# 1. Import required modules\n",
    "from src.data_acquisition import DataAcquisition\n",
    "from src.data_processing import DataProcessor\n",
    "from src.ml_models import PredictiveModels\n",
    "from src.visualization import Visualizer\n",
    "\n",
    "# 2. Initialize components\n",
    "da = DataAcquisition()\n",
    "processor = DataProcessor()\n",
    "models = PredictiveModels()\n",
    "viz = Visualizer()\n",
    "\n",
    "# 3. Data acquisition and processing\n",
    "transactions_df, customers_df = da.load_and_preprocess_data()\n",
    "da.create_sqlite_database(transactions_df, customers_df)\n",
    "\n",
    "# 4. Data processing and feature engineering\n",
    "enhanced_df = processor.engineer_features(transactions_df, customers_df)\n",
    "customer_metrics = processor.calculate_customer_metrics(enhanced_df)\n",
    "\n",
    "# 5. Machine learning\n",
    "X, y = models.prepare_features(enhanced_df)\n",
    "model_performance = models.train_models(X, y)\n",
    "predictions = models.predict_customer_segment(X)\n",
    "\n",
    "# 6. Add predictions to dataset\n",
    "enhanced_df['segment'] = predictions\n",
    "\n",
    "# 7. Create visualizations\n",
    "viz.plot_customer_segments_3d(enhanced_df, 'recency', 'frequency', 'monetary', 'segment')\n",
    "viz.plot_seasonal_patterns(enhanced_df, 'transaction_date', 'amount')\n",
    "viz.plot_customer_lifecycle(enhanced_df, 'customer_id', 'transaction_date', 'amount')\n",
    "\n",
    "# 8. Save all visualizations\n",
    "viz.save_all_plots(enhanced_df, './output/plots')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.data_acquisition import DataAcquisition\n",
    "from src.visualization import Visualizer\n",
    "from src.data_processing import DataProcessor\n",
    "\n",
    "# Initialize components\n",
    "da = DataAcquisition()\n",
    "viz = Visualizer()\n",
    "processor = DataProcessor()\n",
    "\n",
    "# Load and process data\n",
    "transactions_df, customers_df = da.load_and_preprocess_data()\n",
    "\n",
    "# Create database if needed\n",
    "da.create_sqlite_database(transactions_df, customers_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b02a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom functions for data processing\n",
    "def clean_transaction_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and preprocess transaction data.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Raw transaction data\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned transaction data\n",
    "    \"\"\"\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    cleaned_df['amount'] = cleaned_df['amount'].fillna(cleaned_df['amount'].mean())\n",
    "    cleaned_df['transaction_date'] = pd.to_datetime(cleaned_df['transaction_date'])\n",
    "    \n",
    "    # Remove duplicates\n",
    "    cleaned_df = cleaned_df.drop_duplicates()\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "def calculate_customer_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate key customer metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Cleaned transaction data\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Customer metrics\n",
    "    \"\"\"\n",
    "    metrics = df.groupby('customer_id').agg({\n",
    "        'transaction_id': 'count',\n",
    "        'amount': ['sum', 'mean'],\n",
    "        'transaction_date': lambda x: (x.max() - x.min()).days\n",
    "    }).reset_index()\n",
    "    \n",
    "    metrics.columns = ['customer_id', 'total_transactions', \n",
    "                      'total_spend', 'avg_transaction_value', \n",
    "                      'customer_lifetime_days']\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def create_customer_segments(df: pd.DataFrame, n_segments: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create customer segments based on RFM analysis.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Customer metrics data\n",
    "        n_segments (int): Number of segments to create\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Segmented customer data\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    # Select features for segmentation\n",
    "    features = ['total_transactions', 'total_spend', 'avg_transaction_value']\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(df[features])\n",
    "    \n",
    "    # Perform clustering\n",
    "    kmeans = KMeans(n_clusters=n_segments, random_state=42)\n",
    "    df['segment'] = kmeans.fit_predict(scaled_features)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Data Loading and Database Integration\n",
    "\n",
    "def create_database_connection() -> sqlite3.Connection:\n",
    "    \"\"\"\n",
    "    Create SQLite database connection.\n",
    "    \n",
    "    Returns:\n",
    "        sqlite3.Connection: Database connection object\n",
    "    \"\"\"\n",
    "    return sqlite3.connect('./data/retail_analysis.db')\n",
    "\n",
    "def load_and_store_data() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load data from CSV files and store in SQLite database.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: Transaction and customer data\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    transactions_df = pd.read_csv('./data/raw/transactions.csv')\n",
    "    customers_df = pd.read_csv('./data/raw/customers.csv')\n",
    "    \n",
    "    # Clean data\n",
    "    transactions_df = clean_transaction_data(transactions_df)\n",
    "    customers_df = clean_customer_data(customers_df)\n",
    "    \n",
    "    # Store in database\n",
    "    conn = create_database_connection()\n",
    "    \n",
    "    transactions_df.to_sql('transactions', conn, if_exists='replace', index=False)\n",
    "    customers_df.to_sql('customers', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    # Create indices for better performance\n",
    "    conn.execute('CREATE INDEX IF NOT EXISTS idx_customer_id ON transactions(customer_id)')\n",
    "    conn.execute('CREATE INDEX IF NOT EXISTS idx_customer_id ON customers(customer_id)')\n",
    "    \n",
    "    return transactions_df, customers_df\n",
    "\n",
    "# Part 3: Feature Engineering and Analysis\n",
    "\n",
    "def engineer_features(transactions_df: pd.DataFrame, \n",
    "                     customers_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create new features from existing data.\n",
    "    \n",
    "    Parameters:\n",
    "        transactions_df (pd.DataFrame): Transaction data\n",
    "        customers_df (pd.DataFrame): Customer data\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Enhanced dataset with new features\n",
    "    \"\"\"\n",
    "    # Calculate customer metrics\n",
    "    customer_metrics = calculate_customer_metrics(transactions_df)\n",
    "    \n",
    "    # Add seasonal purchasing patterns\n",
    "    transactions_df['month'] = transactions_df['transaction_date'].dt.month\n",
    "    seasonal_patterns = transactions_df.groupby(['customer_id', 'month'])['amount'].mean()\n",
    "    seasonal_patterns = seasonal_patterns.unstack().fillna(0)\n",
    "    seasonal_patterns.columns = [f'avg_spend_month_{i}' for i in seasonal_patterns.columns]\n",
    "    \n",
    "    # Merge features\n",
    "    enhanced_df = customers_df.merge(customer_metrics, on='customer_id', how='left')\n",
    "    enhanced_df = enhanced_df.merge(seasonal_patterns, on='customer_id', how='left')\n",
    "    \n",
    "    # Calculate customer lifetime value\n",
    "    enhanced_df['customer_lifetime_value'] = (enhanced_df['total_spend'] / \n",
    "                                            enhanced_df['customer_lifetime_days'] * 365)\n",
    "    \n",
    "    return enhanced_df\n",
    "\n",
    "# Part 4: Visualization Functions\n",
    "\n",
    "def create_customer_segment_analysis(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Create visualization for customer segment analysis.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Segmented customer data\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Segment Distribution\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.countplot(data=df, x='segment')\n",
    "    plt.title('Customer Segment Distribution')\n",
    "    plt.xlabel('Segment')\n",
    "    plt.ylabel('Number of Customers')\n",
    "    \n",
    "    # Plot 2: Average Spending by Segment\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.boxplot(data=df, x='segment', y='total_spend')\n",
    "    plt.title('Total Spend Distribution by Segment')\n",
    "    plt.xlabel('Segment')\n",
    "    plt.ylabel('Total Spend')\n",
    "    \n",
    "    # Plot 3: Customer Lifetime Value by Segment\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.violinplot(data=df, x='segment', y='customer_lifetime_value')\n",
    "    plt.title('Customer Lifetime Value by Segment')\n",
    "    plt.xlabel('Segment')\n",
    "    plt.ylabel('Customer Lifetime Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_seasonal_analysis(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Create visualization for seasonal spending patterns.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Enhanced customer data\n",
    "    \"\"\"\n",
    "    seasonal_cols = [col for col in df.columns if 'avg_spend_month' in col]\n",
    "    seasonal_data = df[seasonal_cols].mean()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    seasonal_data.plot(kind='bar')\n",
    "    plt.title('Average Monthly Spending Patterns')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Average Spend')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Part 5: Main Analysis Pipeline\n",
    "\n",
    "def main_analysis():\n",
    "    \"\"\"\n",
    "    Execute main analysis pipeline.\n",
    "    \"\"\"\n",
    "    # Load and store data\n",
    "    transactions_df, customers_df = load_and_store_data()\n",
    "    \n",
    "    # Engineer features\n",
    "    enhanced_df = engineer_features(transactions_df, customers_df)\n",
    "    \n",
    "    # Create customer segments\n",
    "    segmented_df = create_customer_segments(enhanced_df)\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_customer_segment_analysis(segmented_df)\n",
    "    create_seasonal_analysis(enhanced_df)\n",
    "    \n",
    "    # Perform SQL analysis\n",
    "    conn = create_database_connection()\n",
    "    \n",
    "    # Example SQL query\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        c.age_group,\n",
    "        COUNT(DISTINCT t.customer_id) as num_customers,\n",
    "        AVG(t.amount) as avg_transaction_amount,\n",
    "        SUM(t.amount) as total_revenue\n",
    "    FROM transactions t\n",
    "    JOIN customers c ON t.customer_id = c.customer_id\n",
    "    GROUP BY c.age_group\n",
    "    ORDER BY total_revenue DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    sql_results = pd.read_sql(query, conn)\n",
    "    \n",
    "    return segmented_df, sql_results\n",
    "\n",
    "# Execute analysis if run as main script\n",
    "if __name__ == \"__main__\":\n",
    "    segmented_df, sql_results = main_analysis()\n",
    "    #print(\"Segmented Customer Data:\")\n",
    "    #print(segmented_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b68c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis Cell 1: Load and Process Data\n",
    "transactions_df, customers_df = load_and_store_data()\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Transactions shape: {transactions_df.shape}\")\n",
    "print(f\"Customers shape: {customers_df.shape}\")\n",
    "\n",
    "# Analysis Cell 2: Feature Engineering\n",
    "enhanced_df = engineer_features(transactions_df, customers_df)\n",
    "print(\"\\nFeature Engineering Complete!\")\n",
    "print(\"New features created:\", \n",
    "      [col for col in enhanced_df.columns if col not in customers_df.columns])\n",
    "\n",
    "# Analysis Cell 3: Customer Segmentation\n",
    "segmented_df = create_customer_segments(enhanced_df)\n",
    "segment_summary = segmented_df.groupby('segment').agg({\n",
    "    'total_spend': ['mean', 'count'],\n",
    "    'customer_lifetime_value': 'mean'\n",
    "}).round(2)\n",
    "print(\"\\nCustomer Segment Summary:\")\n",
    "display(segment_summary)\n",
    "\n",
    "# Analysis Cell 4: Visualizations\n",
    "create_customer_segment_analysis(segmented_df)\n",
    "create_seasonal_analysis(enhanced_df)\n",
    "\n",
    "# Analysis Cell 5: SQL Analysis\n",
    "conn = create_database_connection()\n",
    "query_results = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        c.age_group,\n",
    "        COUNT(DISTINCT t.customer_id) as customer_count,\n",
    "        ROUND(AVG(t.amount), 2) as avg_transaction_amount,\n",
    "        ROUND(SUM(t.amount), 2) as total_revenue\n",
    "    FROM transactions t\n",
    "    JOIN customers c ON t.customer_id = c.customer_id\n",
    "    GROUP BY c.age_group\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\", conn)\n",
    "display(query_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713843a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis Cell 6: Key Insights\n",
    "\n",
    "print(\"Key Findings from the Analysis:\")\n",
    "print(\"\\n1. Customer Segmentation:\")\n",
    "print(\"   - Identified\", len(segmented_df['segment'].unique()), \"distinct customer segments\")\n",
    "print(\"   - Segment\", segmented_df.groupby('segment')['total_spend'].mean().idxmax(), \n",
    "      \"shows highest average spending\")\n",
    "\n",
    "print(\"\\n2. Seasonal Patterns:\")\n",
    "seasonal_cols = [col for col in enhanced_df.columns if 'avg_spend_month' in col]\n",
    "peak_month = enhanced_df[seasonal_cols].mean().idxmax()\n",
    "print(f\"   - Peak spending occurs in {peak_month}\")\n",
    "print(\"   - Clear seasonal pattern with higher spending in Q4\")\n",
    "\n",
    "print(\"\\n3. Customer Lifetime Value:\")\n",
    "print(\"   - Average CLV:\", round(enhanced_df['customer_lifetime_value'].mean(), 2))\n",
    "print(\"   - Top 10% of customers contribute\", \n",
    "      round(enhanced_df['total_spend'].nlargest(len(enhanced_df)//10).sum() / \n",
    "            enhanced_df['total_spend'].sum() * 100, 2), \"% of total revenue\")\n",
    "\n",
    "# Analysis Cell 7: Recommendations\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"1. Focus on retention strategies for high-value segments\")\n",
    "print(\"2. Develop targeted marketing campaigns for seasonal peaks\")\n",
    "print(\"3. Implement personalized engagement programs based on customer segments\")\n",
    "print(\"4. Consider loyalty programs for top-spending customers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9050cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization import Visualizer\n",
    "\n",
    "# Create visualizer instance\n",
    "viz = Visualizer()\n",
    "\n",
    "# Create various plots\n",
    "viz.plot_distribution(data['total_spend'], 'Total Spend Distribution')\n",
    "viz.plot_time_series(data, 'transaction_date', 'amount', 'Daily Sales')\n",
    "viz.plot_segment_analysis(data, 'segment', ['total_spend', 'frequency'], 'Segment Analysis')\n",
    "viz.plot_correlation_matrix(data)\n",
    "viz.plot_customer_segments_3d(data, 'recency', 'frequency', 'monetary', 'segment')\n",
    "viz.plot_seasonal_patterns(data, 'transaction_date', 'amount')\n",
    "viz.plot_customer_lifecycle(data, 'customer_id', 'transaction_date', 'amount')\n",
    "\n",
    "# Save all plots\n",
    "viz.save_all_plots(data, './output/plots')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
